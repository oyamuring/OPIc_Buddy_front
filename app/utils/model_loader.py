"""
AI λ¨λΈ μΈν„°νμ΄μ¤ - λ΅μ»¬/API λ¨λ‘ μ§€μ›
"""
import streamlit as st
import os
from typing import Optional

# ν™κ²½ λ³€μλ΅ λ¨λΈ νƒ€μ… μ„¤μ • (λ‚μ¤‘μ— APIλ΅ μ‰½κ² λ³€κ²½ κ°€λ¥)
MODEL_TYPE = os.getenv("MODEL_TYPE", "local")  # "local" or "api"
API_KEY = os.getenv("OPENAI_API_KEY", "")

def load_model():
    """
    λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤. 
    λ΅μ»¬ λ¨λΈ λλ” API μ„¤μ •μ— λ”°λΌ μ μ ν• μΈν„°νμ΄μ¤λ¥Ό λ°ν™ν•©λ‹λ‹¤.
    """
    if MODEL_TYPE == "api":
        return _load_api_client()
    else:
        return _load_local_model()

@st.cache_resource
def _load_local_model():
    """λ΅μ»¬ FLAN-T5 λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤."""
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
        tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        return {
            "type": "local",
            "pipeline": pipeline("text2text-generation", model=model, tokenizer=tokenizer)
        }
    except Exception as e:
        st.error(f"λ΅μ»¬ λ¨λΈ λ΅λ”© μ¤‘ μ¤λ¥: {e}")
        return None

def _load_api_client():
    """API ν΄λΌμ΄μ–ΈνΈλ¥Ό μ„¤μ •ν•©λ‹λ‹¤. (ν–¥ν›„ OpenAI/λ‹¤λ¥Έ API μ—°λ™μ©)"""
    return {
        "type": "api",
        "api_key": API_KEY,
        "model": "gpt-3.5-turbo"  # λ‚μ¤‘μ— μ‚¬μ©ν•  λ¨λΈ
    }

def generate_response(model_interface: Optional[dict], prompt: str, max_tokens: int = 150) -> str:
    """
    ν†µν•© μ‘λ‹µ μƒμ„± ν•¨μ - λ΅μ»¬/API λ¨λ‘ μ§€μ›
    
    Args:
        model_interface: λ¨λΈ μΈν„°νμ΄μ¤ (λ΅μ»¬ νμ΄ν”„λΌμΈ λλ” API μ„¤μ •)
        prompt: μ™„μ„±λ ν”„λ΅¬ν”„νΈ
        max_tokens: μµλ€ ν† ν° μ
    
    Returns:
        μƒμ„±λ μ‘λ‹µ
    """
    if model_interface is None:
        return "β λ¨λΈμ΄ λ΅λ“λμ§€ μ•μ•μµλ‹λ‹¤."
    
    if model_interface["type"] == "api":
        return _generate_api_response(model_interface, prompt, max_tokens)
    else:
        return _generate_local_response(model_interface, prompt, max_tokens)

def _generate_local_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """λ΅μ»¬ λ¨λΈλ΅ μ‘λ‹µ μƒμ„±"""
    try:
        pipeline_model = model_interface["pipeline"]
        
        # λ” κ°„λ‹¨ν• ν”„λ΅¬ν”„νΈλ΅ λ³€κ²½
        simple_prompt = f"Question: {prompt.split('User Question:')[-1].replace('Response:', '').strip()}\nAnswer:"
        
        response = pipeline_model(
            simple_prompt, 
            max_new_tokens=max_tokens, 
            do_sample=True, 
            temperature=0.7,
            repetition_penalty=1.2,  # λ°λ³µ λ°©μ§€
            num_beams=2
        )
        
        answer = response[0]["generated_text"].replace(simple_prompt, "").strip()
        
        # λ„λ¬΄ μ§§κ±°λ‚ λ°λ³µλλ” λ‹µλ³€ ν•„ν„°λ§
        if not answer or len(answer) < 10 or answer.count(answer.split()[0] if answer.split() else "") > 3:
            return "Hello! I'm here to help you practice English for OPIc. Could you please ask me something specific about English conversation or OPIc preparation?"
        
        return answer
    except Exception as e:
        return f"I'm sorry, I had a technical issue. Could you please try asking your question again? (Error: {str(e)[:50]})"

def _generate_api_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """APIλ΅ μ‘λ‹µ μƒμ„± (ν–¥ν›„ κµ¬ν„ μμ •)"""
    # TODO: OpenAI API λλ” λ‹¤λ¥Έ API μ—°λ™
    return "π§ API λ¨λ“λ” μ•„μ§ κµ¬ν„ μ¤‘μ…λ‹λ‹¤. ν„μ¬λ” λ΅μ»¬ λ¨λΈμ„ μ‚¬μ©ν•΄μ£Όμ„Έμ”."
