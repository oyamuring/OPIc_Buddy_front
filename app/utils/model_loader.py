"""
AI λ¨λΈ μΈν„°νμ΄μ¤ - λ΅μ»¬/API λ¨λ‘ μ§€μ›
"""
import streamlit as st
import os
from typing import Optional

# ν™κ²½ λ³€μλ΅ λ¨λΈ νƒ€μ… μ„¤μ • (λ‚μ¤‘μ— APIλ΅ μ‰½κ² λ³€κ²½ κ°€λ¥)
MODEL_TYPE = os.getenv("MODEL_TYPE", "local")  # "local" or "api"
API_KEY = os.getenv("OPENAI_API_KEY", "")

def load_model():
    """
    λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤. 
    λ΅μ»¬ λ¨λΈ λλ” API μ„¤μ •μ— λ”°λΌ μ μ ν• μΈν„°νμ΄μ¤λ¥Ό λ°ν™ν•©λ‹λ‹¤.
    """
    if MODEL_TYPE == "api":
        return _load_api_client()
    else:
        return _load_local_model()

@st.cache_resource
def _load_local_model():
    """λ΅μ»¬ FLAN-T5 λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤."""
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
        tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
        model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
        return {
            "type": "local",
            "pipeline": pipeline("text2text-generation", model=model, tokenizer=tokenizer)
        }
    except Exception as e:
        st.error(f"λ΅μ»¬ λ¨λΈ λ΅λ”© μ¤‘ μ¤λ¥: {e}")
        return None

def _load_api_client():
    """API ν΄λΌμ΄μ–ΈνΈλ¥Ό μ„¤μ •ν•©λ‹λ‹¤. (ν–¥ν›„ OpenAI/λ‹¤λ¥Έ API μ—°λ™μ©)"""
    return {
        "type": "api",
        "api_key": API_KEY,
        "model": "gpt-3.5-turbo"  # λ‚μ¤‘μ— μ‚¬μ©ν•  λ¨λΈ
    }

def generate_response(model_interface: Optional[dict], prompt: str, max_tokens: int = 150) -> str:
    """
    ν†µν•© μ‘λ‹µ μƒμ„± ν•¨μ - λ΅μ»¬/API λ¨λ‘ μ§€μ›
    
    Args:
        model_interface: λ¨λΈ μΈν„°νμ΄μ¤ (λ΅μ»¬ νμ΄ν”„λΌμΈ λλ” API μ„¤μ •)
        prompt: μ™„μ„±λ ν”„λ΅¬ν”„νΈ
        max_tokens: μµλ€ ν† ν° μ
    
    Returns:
        μƒμ„±λ μ‘λ‹µ
    """
    if model_interface is None:
        return "β λ¨λΈμ΄ λ΅λ“λμ§€ μ•μ•μµλ‹λ‹¤."
    
    if model_interface["type"] == "api":
        return _generate_api_response(model_interface, prompt, max_tokens)
    else:
        return _generate_local_response(model_interface, prompt, max_tokens)

def _generate_local_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """λ΅μ»¬ λ¨λΈλ΅ μ‘λ‹µ μƒμ„±"""
    try:
        pipeline_model = model_interface["pipeline"]
        response = pipeline_model(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)
        answer = response[0]["generated_text"].replace(prompt, "").strip()
        
        if not answer:
            answer = "π¤– μ£„μ†΅ν•΄μ”, λ‹µλ³€μ„ μƒμ„±ν•μ§€ λ»ν–μ–΄μ”. μ§λ¬Έμ„ λ” κµ¬μ²΄μ μΌλ΅ ν•΄μ£Όμ„Έμ”!"
        
        return answer
    except Exception as e:
        return f"β λ΅μ»¬ λ¨λΈ μ‘λ‹µ μƒμ„± μ¤λ¥: {e}"

def _generate_api_response(model_interface: dict, prompt: str, max_tokens: int) -> str:
    """APIλ΅ μ‘λ‹µ μƒμ„± (ν–¥ν›„ κµ¬ν„ μμ •)"""
    # TODO: OpenAI API λλ” λ‹¤λ¥Έ API μ—°λ™
    return "π§ API λ¨λ“λ” μ•„μ§ κµ¬ν„ μ¤‘μ…λ‹λ‹¤. ν„μ¬λ” λ΅μ»¬ λ¨λΈμ„ μ‚¬μ©ν•΄μ£Όμ„Έμ”."
