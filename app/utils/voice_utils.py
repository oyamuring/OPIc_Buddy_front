"""
OPIc ì‹œí—˜ìš© í†µí•© ìŒì„± ê¸°ëŠ¥ ìœ í‹¸ë¦¬í‹°
- TTS: ë¬¸ì œ ì½ì–´ì£¼ê¸° (gTTS)
- STT: OpenAI Whisper API ì‚¬ìš©
- ìŒì„± ì¸ì‹ ì¸í„°í˜ì´ìŠ¤
- íƒ€ì´ë¨¸: 1ë¶„ ì œí•œ
"""

import io
import os
import time
import tempfile
import streamlit as st
import speech_recognition as sr
from gtts import gTTS
from openai import OpenAI

class VoiceManager:
    def __init__(self):
        # OpenAI API í‚¤ í™•ì¸
        self.openai_client = None
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = OpenAI(api_key=api_key)
        
    def text_to_speech(self, text: str, lang: str = 'en') -> bytes:
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        try:
            tts = gTTS(text=text, lang=lang, slow=False)
            fp = io.BytesIO()
            tts.write_to_fp(fp)
            fp.seek(0)
            return fp.getvalue()
        except Exception as e:
            st.error(f"TTS ì˜¤ë¥˜: {str(e)}")
            return None

    def play_question_audio(self, question_text: str):
        """ì§ˆë¬¸ì„ ìŒì„±ìœ¼ë¡œ ì¬ìƒ"""
        audio_data = self.text_to_speech(question_text)
        if audio_data:
            st.audio(audio_data, format='audio/mp3')
            st.success("ğŸ”Š ë¬¸ì œë¥¼ ì¬ìƒí•©ë‹ˆë‹¤!")

    def speech_to_text(self, audio_bytes: bytes) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (OpenAI Whisper API ì‚¬ìš©)"""
        if not self.openai_client:
            st.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ STT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "[Voice recording - STT unavailable]"
        
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì˜¤ë””ì˜¤ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                tmp_file.write(audio_bytes)
                tmp_file.flush()
                
                # OpenAI Whisper APIë¡œ STT ìˆ˜í–‰
                with open(tmp_file.name, "rb") as audio_file:
                    transcript = self.openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language="en"  # ì˜ì–´ë¡œ ê³ ì •
                    )
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file.name)
                
                return transcript.text.strip()
                
        except Exception as e:
            st.error(f"STT ì˜¤ë¥˜: {str(e)}")
            return f"[Voice recording - STT error: {str(e)}]"

def unified_answer_input(question_idx: int, question_text: str) -> str:
    """í†µí•©ëœ ë‹µë³€ ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ (ìŒì„± + í…ìŠ¤íŠ¸)"""
    
    voice_manager = VoiceManager()
    
    # í˜„ì¬ ì €ì¥ëœ ë‹µë³€ í™•ì¸
    answer_key = f"ans_{question_idx}"
    current_answer = st.session_state.get(answer_key, "")
    
    # íƒ­ìœ¼ë¡œ ì…ë ¥ ë°©ì‹ êµ¬ë¶„
    tab1, tab2 = st.tabs(["ğŸ¤ ìŒì„± ë‹µë³€", "ğŸ’¬ í…ìŠ¤íŠ¸ ë‹µë³€"])
    
    final_answer = ""
    
    with tab1:
        st.markdown("#### ğŸ¤ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•˜ê¸° (ìµœëŒ€ 60ì´ˆ)")
        
        # ë§ˆì´í¬ ì‚¬ìš© ì•ˆë‚´ (í•œ ë²ˆë§Œ í‘œì‹œ)
        if not st.session_state.get("voice_instructions_shown", False):
            st.info("""
            ğŸ” **ë§ˆì´í¬ ì‚¬ìš© ë°©ë²•:**
            1. ì•„ë˜ ë§ˆì´í¬ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            2. ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”
            3. ë…¹ìŒ ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ì˜ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
            4. ë‹¤ì‹œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ë…¹ìŒì„ ì¢…ë£Œí•˜ì„¸ìš”
            
            âš ï¸ Chrome, Safari, Firefox ë“± ìµœì‹  ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
            """)
            st.session_state.voice_instructions_shown = True
        
        # ê¸°ì¡´ ë‹µë³€ì´ ìˆë‹¤ë©´ í‘œì‹œ (ë‹¨, ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ì˜¨ ê²½ìš°ëŠ” ìˆ¨ê¹€)
        hide_flag = st.session_state.get(f"hide_current_answer_{question_idx}", False)
        if hide_flag:
            # í”Œë˜ê·¸ë¥¼ í•œ ë²ˆ ì‚¬ìš©í•˜ë©´ ì‚­ì œ
            del st.session_state[f"hide_current_answer_{question_idx}"]
        elif current_answer and not current_answer.startswith("[Voice"):
            st.success(f"ğŸ’­ í˜„ì¬ í…ìŠ¤íŠ¸ ë‹µë³€:")
            st.info(current_answer)
        elif current_answer:
            st.success(f"âœ… ìŒì„± ë‹µë³€ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        # ì˜¤ë””ì˜¤ ì…ë ¥ (ê°œì„ ëœ ë²„ì „)
        audio_value = st.audio_input(
            "ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ë…¹ìŒì„ ì‹œì‘/ì¢…ë£Œí•˜ì„¸ìš”",
            key=f"audio_input_{question_idx}",
            help="ë§ˆì´í¬ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒì„ ì‹œì‘í•˜ê³ , ë‹¤ì‹œ í´ë¦­í•˜ì—¬ ì¢…ë£Œí•˜ì„¸ìš”. ìµœëŒ€ 60ì´ˆê¹Œì§€ ë…¹ìŒ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
        
        if audio_value is not None:
            st.success("ğŸµ ìŒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ë…¹ìŒë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì¬ìƒ
            st.audio(audio_value, format='audio/wav')
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (Next ë²„íŠ¼ í´ë¦­ ì‹œ ìë™ ë³€í™˜ìš©)
            st.session_state[f"audio_data_{question_idx}"] = audio_value.getvalue()
            
            # ë‹µë³€ ë³€í™˜ ë° í™•ì¸ ë²„íŠ¼
            if st.button("ë‚´ ë‹µë³€ ë³´ê¸°", key=f"stt_btn_{question_idx}"):
                with st.spinner("ğŸ”„ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
                    transcript = voice_manager.speech_to_text(audio_value.getvalue())
                
                if transcript and not transcript.startswith("[Voice recording"):
                    final_answer = transcript
                    st.session_state[answer_key] = final_answer
                    # ìƒˆë¡œ ë³€í™˜ëœ ë‹µë³€ì´ ë°”ë¡œ í‘œì‹œë˜ë„ë¡ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                    st.rerun()
                else:
                    st.error("âš ï¸ ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë…¹ìŒí•´ë³´ì„¸ìš”.")
        else:
            pass
    with tab2:
        st.markdown("#### ğŸ’¬ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í•˜ê¸°")
        text_answer = st.text_area(
            "Your answer (English):",
            value=current_answer if not current_answer.startswith("[Voice") else "",
            key=f"text_input_{question_idx}",
            height=150,
            help="ì˜ì–´ë¡œ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ìŒì„± ë‹µë³€ ëŒ€ì‹  ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        if text_answer.strip():
            final_answer = text_answer.strip()
            st.session_state[answer_key] = final_answer
    
    return st.session_state.get(answer_key, "")

def auto_convert_audio_if_needed(question_idx: int) -> str:
    """Next ë²„íŠ¼ í´ë¦­ ì‹œ ë…¹ìŒëœ ìŒì„±ì„ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    answer_key = f"ans_{question_idx}"
    audio_key = f"audio_data_{question_idx}"
    
    # ì´ë¯¸ ë‹µë³€ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    existing_answer = st.session_state.get(answer_key, "")
    if existing_answer and not existing_answer.startswith("[Voice recording"):
        return existing_answer
    
    # ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ìˆê³  ì•„ì§ ë³€í™˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìë™ ë³€í™˜
    audio_data = st.session_state.get(audio_key)
    if audio_data:
        try:
            voice_manager = VoiceManager()
            
            # STT ë³€í™˜ ìˆ˜í–‰
            transcript = voice_manager.speech_to_text(audio_data)
            
            if transcript and not transcript.startswith("[Voice recording"):
                # ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹µë³€ìœ¼ë¡œ ì €ì¥
                st.session_state[answer_key] = transcript
                # ì˜¤ë””ì˜¤ ë°ì´í„°ëŠ” í”¼ë“œë°±ì—ì„œ ì¬ìƒí•˜ê¸° ìœ„í•´ ìœ ì§€ (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
                # í”¼ë“œë°± í˜ì´ì§€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì¶”ê°€ í‚¤ë¡œë„ ì €ì¥
                st.session_state[f"audio_{question_idx}"] = audio_data
                return transcript
            else:
                return "[Voice recording - conversion failed]"
        except Exception as e:
            return f"[Voice recording - STT error: {str(e)}]"
    
    return existing_answer

# ========== ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ==========

def display_tts_button(text, message_index=0):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë²„íŠ¼ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ìš°ì¸¡ ì •ë ¬ì„ ìœ„í•œ ì»¬ëŸ¼ ì‚¬ìš©
    col1, col2 = st.columns([2.5, 1.5])
    
    with col2:
        # ë©”ì‹œì§€ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ ìœ ë‹ˆí¬í•œ í‚¤ ìƒì„±
        unique_key = f"tts_{message_index}_{hash(text)}"
        if st.button("ğŸ”Š ìŒì„±ìœ¼ë¡œ ë“£ê¸°", key=unique_key, 
                     help="ìŒì„±ìœ¼ë¡œ ì¬ìƒí•˜ê¸°",
                     use_container_width=True):
            _generate_google_tts(text)

def _generate_google_tts(text, lang="en"):
    """Google TTSë¡œ ë¹ ë¥¸ ìŒì„± ìƒì„±"""
    try:
        # í…ìŠ¤íŠ¸ ê²€ì¦
        if not text or len(text.strip()) == 0:
            st.error("ì¬ìƒí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        if len(text) > 500:
            text = text[:500] + "..."
            st.info("í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ 500ìê¹Œì§€ë§Œ ì¬ìƒë©ë‹ˆë‹¤.")
            
        with st.spinner("ğŸµ ìŒì„± ìƒì„± ì¤‘..."):
            # Google TTSë¡œ ìŒì„± ìƒì„±
            tts = gTTS(text=text, lang=lang, slow=False)
            
            # ì„ì‹œ íŒŒì¼ ë°©ì‹ ì‚¬ìš©
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
                tts.save(tmp_file.name)
                
                # íŒŒì¼ ì½ê¸°
                with open(tmp_file.name, 'rb') as audio_file:
                    audio_bytes = audio_file.read()
                    
                # Streamlitì—ì„œ ìŒì„± ì¬ìƒ
                st.audio(audio_bytes, format='audio/mp3')
                st.success("ğŸ§ ìŒì„± ì¬ìƒ ì¤€ë¹„ ì™„ë£Œ!")
                
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file.name)
            
    except Exception as e:
        st.error(f"ìŒì„± ìƒì„± ì˜¤ë¥˜: {e}")
        st.info("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

def recognize_speech():
    """
    ë§ˆì´í¬ë¡œë¶€í„° ìŒì„±ì„ ì¸ì‹í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        tuple: (ì„±ê³µ ì—¬ë¶€, ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€)
    """
    try:
        recognizer = sr.Recognizer()
        
        with sr.Microphone() as source:
            st.info("ğŸ§ ë§í•´ì£¼ì„¸ìš” (ìµœëŒ€ 60ì´ˆ)...")
            audio = recognizer.listen(source, timeout=10, phrase_time_limit=60)
            st.info("ğŸ§  ìŒì„± ì¸ì‹ ì¤‘...")
        
        # Google STT ì‚¬ìš©
        question = recognizer.recognize_google(audio, language="en-US")
        return True, question
        
    except sr.UnknownValueError:
        return False, "ğŸ˜µ ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except sr.RequestError as e:
        return False, f"ğŸ”Œ Google STT ìš”ì²­ ì‹¤íŒ¨: {e}"
    except Exception as e:
        return False, f"âš ï¸ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}"

def display_speech_interface():
    """ìŒì„± ì¸ì‹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ë™ì¼í•œ í¬ê¸°ë¡œ ë²„íŠ¼ í‘œì‹œ
    col1, col2 = st.columns([2.5, 1.5])
    
    with col2:
        if st.button("ğŸ¤ ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°", key="speech_input", use_container_width=True):
            with st.spinner("ìŒì„± ì…ë ¥ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."):
                success, text = recognize_speech()
                
            if success:
                st.success(f"âœ… ì¸ì‹ëœ ì§ˆë¬¸: {text}")
                st.session_state.user_input = text
            else:
                st.error(text)
