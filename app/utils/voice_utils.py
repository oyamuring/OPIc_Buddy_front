"""
OPIc ì‹œí—˜ìš© í†µí•© ìŒì„± ê¸°ëŠ¥ ìœ í‹¸ë¦¬í‹°
- TTS: ë¬¸ì œ ì½ì–´ì£¼ê¸° (gTTS)
- STT: OpenAI Whisper API ì‚¬ìš©
- ìŒì„± ì¸ì‹ ì¸í„°í˜ì´ìŠ¤
- íƒ€ì´ë¨¸: 1ë¶„ ì œí•œ
"""

import io
import os
import time
import tempfile
import streamlit as st
## import speech_recognition as sr  # ì œê±°
## from gtts import gTTS  # ì œê±°
from openai import OpenAI

class VoiceManager:
    def __init__(self):
        # OpenAI API í‚¤ í™•ì¸
        self.openai_client = None
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = OpenAI(api_key=api_key)
        
    # def text_to_speech(self, text: str, lang: str = 'en') -> bytes:
    #     """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
    #     try:
    #         tts = gTTS(text=text, lang=lang, slow=False)
    #         fp = io.BytesIO()
    #         tts.write_to_fp(fp)
    #         fp.seek(0)
    #         return fp.getvalue()
    #     except Exception as e:
    #         st.error(f"TTS ì˜¤ë¥˜: {str(e)}")
    #         return None

    # def play_question_audio(self, question_text: str):
    #     """ì§ˆë¬¸ì„ ìŒì„±ìœ¼ë¡œ ì¬ìƒ"""
    #     audio_data = self.text_to_speech(question_text)
    #     if audio_data:
    #         st.audio(audio_data, format='audio/mp3')
    #         st.success("ğŸ”Š ë¬¸ì œë¥¼ ì¬ìƒí•©ë‹ˆë‹¤!")

    def speech_to_text(self, audio_bytes: bytes) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (OpenAI Whisper API ì‚¬ìš©)"""
        if not self.openai_client:
            st.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ STT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "[Voice recording - STT unavailable]"
        
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì˜¤ë””ì˜¤ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                tmp_file.write(audio_bytes)
                tmp_file.flush()
                
                # OpenAI Whisper APIë¡œ STT ìˆ˜í–‰
                with open(tmp_file.name, "rb") as audio_file:
                    transcript = self.openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language="en"  # ì˜ì–´ë¡œ ê³ ì •
                    )
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file.name)
                
                return transcript.text.strip()
                
        except Exception as e:
            st.error(f"STT ì˜¤ë¥˜: {str(e)}")
            return f"[Voice recording - STT error: {str(e)}]"

def unified_answer_input(question_idx: int, question_text: str) -> str:
    """í†µí•©ëœ ë‹µë³€ ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ (ìŒì„± + í…ìŠ¤íŠ¸)"""
    
    voice_manager = VoiceManager()
    
    # í˜„ì¬ ì €ì¥ëœ ë‹µë³€ í™•ì¸
    answer_key = f"ans_{question_idx}"
    current_answer = st.session_state.get(answer_key, "")
    
    # íƒ­ìœ¼ë¡œ ì…ë ¥ ë°©ì‹ êµ¬ë¶„
    tab1, tab2 = st.tabs(["ğŸ¤ ìŒì„± ë‹µë³€", "ğŸ’¬ í…ìŠ¤íŠ¸ ë‹µë³€"])
    
    final_answer = ""
    
    with tab1:
        st.markdown("#### ğŸ¤ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•˜ê¸° (ìµœëŒ€ 60ì´ˆ)")
        
        # ë§ˆì´í¬ ì‚¬ìš© ì•ˆë‚´ (í•œ ë²ˆë§Œ í‘œì‹œ)
        if not st.session_state.get("voice_instructions_shown", False):
            st.info("""
            ğŸ” **ë§ˆì´í¬ ì‚¬ìš© ë°©ë²•:**
            1. ì•„ë˜ ë§ˆì´í¬ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            2. ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”
            3. ë…¹ìŒ ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ì˜ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
            4. ë‹¤ì‹œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ë…¹ìŒì„ ì¢…ë£Œí•˜ì„¸ìš”
            
            âš ï¸ Chrome, Safari, Firefox ë“± ìµœì‹  ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
            """)
            st.session_state.voice_instructions_shown = True
        
        # ê¸°ì¡´ ë‹µë³€ì´ ìˆë‹¤ë©´ í‘œì‹œ (ë‹¨, ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ì˜¨ ê²½ìš°ëŠ” ìˆ¨ê¹€)
        hide_flag = st.session_state.get(f"hide_current_answer_{question_idx}", False)
        if hide_flag:
            # í”Œë˜ê·¸ë¥¼ í•œ ë²ˆ ì‚¬ìš©í•˜ë©´ ì‚­ì œ
            del st.session_state[f"hide_current_answer_{question_idx}"]
        elif current_answer and not current_answer.startswith("[Voice"):
            st.success(f"ğŸ’­ í˜„ì¬ í…ìŠ¤íŠ¸ ë‹µë³€:")
            st.info(current_answer)
        elif current_answer:
            st.success(f"âœ… ìŒì„± ë‹µë³€ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        # ì˜¤ë””ì˜¤ ì…ë ¥ (ê°œì„ ëœ ë²„ì „)
    # audio_value = st.audio_input(
    #     "ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ë…¹ìŒì„ ì‹œì‘/ì¢…ë£Œí•˜ì„¸ìš”",
    #     key=f"audio_input_{question_idx}",
    #     help="ë§ˆì´í¬ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë…¹ìŒì„ ì‹œì‘í•˜ê³ , ë‹¤ì‹œ í´ë¦­í•˜ì—¬ ì¢…ë£Œí•˜ì„¸ìš”. ìµœëŒ€ 60ì´ˆê¹Œì§€ ë…¹ìŒ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    # )
        
    # audio_value ê´€ë ¨ ì½”ë“œëŠ” speech_recognition/gTTS ì œê±°ë¡œ ì¸í•´ ë¹„í™œì„±í™”
    # ìŒì„± ë³€í™˜ ì‹¤íŒ¨/ì„±ê³µ ë¶„ê¸° ë° audio_value ê´€ë ¨ ì½”ë“œ ì „ì²´ ì œê±°
    with tab2:
        st.markdown("#### ğŸ’¬ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í•˜ê¸°")
        # ë™ì  í‚¤ ì ìš©: exam.pyì—ì„œ text_input_key_{question_idx}ê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        text_input_key = st.session_state.get(f"text_input_key_{question_idx}", f"text_input_{question_idx}")
        text_answer = st.text_area(
            "Your answer (English):",
            value=current_answer if not current_answer.startswith("[Voice") else "",
            key=text_input_key,
            height=150,
            help="ì˜ì–´ë¡œ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ìŒì„± ë‹µë³€ ëŒ€ì‹  ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        if text_answer.strip():
            final_answer = text_answer.strip()
            st.session_state[answer_key] = final_answer
    
    return st.session_state.get(answer_key, "")

def auto_convert_audio_if_needed(question_idx: int) -> str:
    """Next ë²„íŠ¼ í´ë¦­ ì‹œ ë…¹ìŒëœ ìŒì„±ì„ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    answer_key = f"ans_{question_idx}"
    audio_key = f"audio_data_{question_idx}"
    
    # ì´ë¯¸ ë‹µë³€ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    existing_answer = st.session_state.get(answer_key, "")
    if existing_answer and not existing_answer.startswith("[Voice recording"):
        return existing_answer
    
    # ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ìˆê³  ì•„ì§ ë³€í™˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìë™ ë³€í™˜
    audio_data = st.session_state.get(audio_key)
    if audio_data:
        try:
            voice_manager = VoiceManager()
            
            # STT ë³€í™˜ ìˆ˜í–‰
            transcript = voice_manager.speech_to_text(audio_data)
            
            if transcript and not transcript.startswith("[Voice recording"):
                # ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹µë³€ìœ¼ë¡œ ì €ì¥
                st.session_state[answer_key] = transcript
                # ì˜¤ë””ì˜¤ ë°ì´í„°ëŠ” í”¼ë“œë°±ì—ì„œ ì¬ìƒí•˜ê¸° ìœ„í•´ ìœ ì§€ (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
                # í”¼ë“œë°± í˜ì´ì§€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì¶”ê°€ í‚¤ë¡œë„ ì €ì¥
                st.session_state[f"audio_{question_idx}"] = audio_data
                return transcript
            else:
                return "[Voice recording - conversion failed]"
        except Exception as e:
            return f"[Voice recording - STT error: {str(e)}]"
    
    return existing_answer

# ========== ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ==========

def display_tts_button(text, message_index=0):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë²„íŠ¼ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ìš°ì¸¡ ì •ë ¬ì„ ìœ„í•œ ì»¬ëŸ¼ ì‚¬ìš©
    col1, col2 = st.columns([2.5, 1.5])
    
    with col2:
        # ë©”ì‹œì§€ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ ìœ ë‹ˆí¬í•œ í‚¤ ìƒì„±
        unique_key = f"tts_{message_index}_{hash(text)}"
        if st.button("ğŸ”Š ìŒì„±ìœ¼ë¡œ ë“£ê¸°", key=unique_key, 
                     help="ìŒì„±ìœ¼ë¡œ ì¬ìƒí•˜ê¸°",
                     use_container_width=True):
            # _generate_google_tts(text)  # gTTS ì œê±°ë¡œ ë¹„í™œì„±í™”

## def _generate_google_tts(text, lang="en"):
##     """Google TTSë¡œ ë¹ ë¥¸ ìŒì„± ìƒì„±"""
##     ... (gTTS ê´€ë ¨ ì½”ë“œ ì „ì²´ ì£¼ì„ ì²˜ë¦¬)

## def recognize_speech():
##     ... (speech_recognition ê´€ë ¨ ì½”ë“œ ì „ì²´ ì£¼ì„ ì²˜ë¦¬)

## def display_speech_interface():
##     pass  # (speech_recognition ê´€ë ¨ ì½”ë“œ ì „ì²´ ì£¼ì„ ì²˜ë¦¬)